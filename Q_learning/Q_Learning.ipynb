{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_rows = 4\n",
    "environment_columns = 6\n",
    "q_values = np.zeros((environment_rows, environment_columns, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numeric action codes: 0 = up, 1 = right, 2 = down, 3 = left\n",
    "actions = ['up', 'right', 'down', 'left']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 1.]\n",
      "[0. 0. 0. 0. 0. 0.]\n",
      "[ 0.  0.  0.  0. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "rewards = np.full((environment_rows, environment_columns), 0.)\n",
    "rewards[1, 5] = 1. \n",
    "\n",
    "rewards[3,4] = -1 #trap position\n",
    "rewards[3,5] = -1\n",
    "\n",
    "for row in rewards:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that determines if the specified location is a terminal state\n",
    "def is_terminal_state(current_row_index, current_column_index):\n",
    "  #if the reward for this location is -1, then it is not a terminal state (i.e., it is a 'white square')\n",
    "  if rewards[current_row_index, current_column_index] == 0:\n",
    "    return False\n",
    "  else:\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that will choose a random, non-terminal starting location\n",
    "def get_starting_location():\n",
    "  #get a random row and column index\n",
    "  current_row_index = np.random.randint(2)\n",
    "  if current_row_index == 0:\n",
    "    current_column_index = 0\n",
    "  elif current_row_index == 1:\n",
    "      current_column_index = 2\n",
    "  return current_row_index, current_column_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define an epsilon greedy algorithm that will choose which action to take next (i.e., where to move next)\n",
    "def get_next_action(current_row_index, current_column_index, epsilon):\n",
    "  if np.random.random() < epsilon:\n",
    "    return np.argmax(q_values[current_row_index, current_column_index])\n",
    "  else: #choose a random action\n",
    "    return np.random.randint(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a function that will get the next location based on the chosen action\n",
    "def get_next_location(current_row_index, current_column_index, action_index):\n",
    "  new_row_index = current_row_index\n",
    "  new_column_index = current_column_index\n",
    "  if actions[action_index] == 'up' and current_row_index > 0:\n",
    "    new_row_index -= 1\n",
    "  elif actions[action_index] == 'right' and current_column_index < environment_columns - 1:\n",
    "    new_column_index += 1\n",
    "  elif actions[action_index] == 'down' and current_row_index < environment_rows - 1:\n",
    "    new_row_index += 1\n",
    "  elif actions[action_index] == 'left' and current_column_index > 0:\n",
    "    new_column_index -= 1\n",
    "  return new_row_index, new_column_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shortest_path(current_row_index, current_column_index):\n",
    "    shortest_path = []\n",
    "    steps = 0\n",
    "    shortest_path.append([current_row_index, current_column_index])\n",
    "    reward = 1\n",
    "    while not is_terminal_state(current_row_index, current_column_index) or steps < 20:\n",
    "      if (current_row_index == 1 and current_column_index ==5):\n",
    "        reward += 1\n",
    "      action_index = get_next_action(current_row_index, current_column_index, 1.)\n",
    "      current_row_index, current_column_index = get_next_location(current_row_index, current_column_index, action_index)\n",
    "      shortest_path.append([current_row_index, current_column_index])\n",
    "      steps += 1\n",
    "    return shortest_path, steps, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q Learning complete!\n"
     ]
    }
   ],
   "source": [
    "epsilon = 0.9 \n",
    "discount_factor = 0.9 #discount factor for future rewards\n",
    "learning_rate = 0.9 #the rate at which the AI agent should learn\n",
    "reward_together = []\n",
    "#run through 1000 training episodes\n",
    "for episode in range(1000):\n",
    "  row_index, column_index = get_starting_location()\n",
    "  while not is_terminal_state(row_index, column_index):\n",
    "    action_index = get_next_action(row_index, column_index, epsilon)\n",
    "    old_row_index, old_column_index = row_index, column_index #store the old row and column indexes\n",
    "    row_index, column_index = get_next_location(row_index, column_index, action_index)\n",
    "    \n",
    "    #receive the reward for moving to the new state, and calculate the temporal difference\n",
    "    reward = rewards[row_index, column_index]\n",
    "    reward_together.append(reward)\n",
    "    \n",
    "    #print(\"the reward is: \", reward)\n",
    "    old_q_value = q_values[old_row_index, old_column_index, action_index]\n",
    "    temporal_difference = reward + (discount_factor * np.max(q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "    #update the Q-value for the previous state and action pair\n",
    "    new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "    q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "\n",
    "\n",
    "with open('reward.csv', 'w', newline='') as f:\n",
    "  header = ['x','y']\n",
    "  writer = csv.writer(f)\n",
    "  writer.writerow(header)\n",
    "  for i in range(len(reward_together)):\n",
    "    writer.writerow([i,reward_together[i]])\n",
    "    \n",
    "#print(len(reward_together)) \n",
    "\n",
    "\n",
    "#df  = pd.read_csv(\"reward.csv\")\n",
    "#df.plot(kind='scatter',x='x',y='y') # scatter plot\n",
    "\n",
    "\n",
    "print('Q Learning complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Point:  1 2\n"
     ]
    }
   ],
   "source": [
    "test_r, test_c = get_starting_location()\n",
    "print(\"Starting Point: \", test_r,test_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Steps:  21\n",
      "Max Rewards:  9\n",
      "Route:  [[1, 2], [0, 2], [0, 3], [0, 4], [0, 5], [1, 5], [0, 5], [1, 5], [0, 5], [1, 5], [0, 5], [1, 5], [0, 5], [1, 5], [0, 5], [1, 5], [0, 5], [1, 5], [0, 5], [1, 5], [0, 5], [1, 5]]\n"
     ]
    }
   ],
   "source": [
    "result = get_shortest_path(test_r,test_c) #starting at row 5, column 0\n",
    "print(\"Total Steps: \", result[1])\n",
    "print(\"Max Rewards: \", result[2])\n",
    "print(\"Route: \", result[0])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
